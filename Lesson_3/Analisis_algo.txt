Сравнительный анализ алгоритмов Adam, RMSProp и SGD.

Самая медленная скорость обучения демонстрирует SGD, но и до процесса переобучения я с ним не дошел,
видимо требуется большее кол-во эпох обучения.

Лучшие показатели традиционно демонстрировал Adam, но не сильно ему уступал RMSProp.
Оба метода легко достигали порога переобучения.

Все методы лучших показателей достигали без применения BatchNorm слоев и Dropout'а.

Замечу, что не один метод не демонстрировал стабильных показателей при прочих равных входных значений параметров.
